{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87171,"databundleVersionId":9897270,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-10T06:40:37.681215Z","iopub.execute_input":"2024-11-10T06:40:37.682109Z","iopub.status.idle":"2024-11-10T06:40:37.698867Z","shell.execute_reply.started":"2024-11-10T06:40:37.682062Z","shell.execute_reply":"2024-11-10T06:40:37.697243Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/iml-fall-2024-challenge-1/sample_submission.csv\n/kaggle/input/iml-fall-2024-challenge-1/test_set.csv\n/kaggle/input/iml-fall-2024-challenge-1/train_set.csv\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import pandas as pd\ntrain_set_path = '/kaggle/input/iml-fall-2024-challenge-1/train_set.csv'\ntrain_set = pd.read_csv(train_set_path)\n# drop recordid column as its irrevelant\ntrain_set.drop(columns=['RecordId','X71','X76'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:40:37.705899Z","iopub.execute_input":"2024-11-10T06:40:37.706325Z","iopub.status.idle":"2024-11-10T06:40:40.818584Z","shell.execute_reply.started":"2024-11-10T06:40:37.706285Z","shell.execute_reply":"2024-11-10T06:40:40.817390Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# splitting into input features and target variable\nX = train_set.drop(columns=['Y'])\nY = train_set['Y'] ","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:40.820211Z","iopub.execute_input":"2024-11-10T06:40:40.820557Z","iopub.status.idle":"2024-11-10T06:40:40.871639Z","shell.execute_reply.started":"2024-11-10T06:40:40.820523Z","shell.execute_reply":"2024-11-10T06:40:40.870499Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimport numpy as np\nimr = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_imputed = imr.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:40.874822Z","iopub.execute_input":"2024-11-10T06:40:40.875325Z","iopub.status.idle":"2024-11-10T06:40:41.317018Z","shell.execute_reply.started":"2024-11-10T06:40:40.875273Z","shell.execute_reply":"2024-11-10T06:40:41.315962Z"},"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nX_norm = mms.fit_transform(X_imputed)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:41.318548Z","iopub.execute_input":"2024-11-10T06:40:41.319141Z","iopub.status.idle":"2024-11-10T06:40:41.459453Z","shell.execute_reply.started":"2024-11-10T06:40:41.319076Z","shell.execute_reply":"2024-11-10T06:40:41.458245Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# splitting into training and validating for model selection\nfrom sklearn.model_selection import train_test_split\nX_train, X_validate, Y_train, Y_validate = train_test_split(X_norm,Y,test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:41.460748Z","iopub.execute_input":"2024-11-10T06:40:41.461167Z","iopub.status.idle":"2024-11-10T06:40:41.715289Z","shell.execute_reply.started":"2024-11-10T06:40:41.461127Z","shell.execute_reply":"2024-11-10T06:40:41.714096Z"},"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import lightgbm as lgb\nlgb_model = lgb.LGBMClassifier(max_depth=10, n_estimators=290, learning_rate=0.025, colsample_bytree=0.19, min_child_weight=2, reg_alpha=0.19, reg_lambda=0.19, random_state=42)\nlgb_model.fit(X_train, Y_train)\nmd_predictions_probs = lgb_model.predict_proba(X_validate)\nmd_predictions_probs","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:41.716797Z","iopub.execute_input":"2024-11-10T06:40:41.717277Z","iopub.status.idle":"2024-11-10T06:40:58.166676Z","shell.execute_reply.started":"2024-11-10T06:40:41.717227Z","shell.execute_reply":"2024-11-10T06:40:58.165417Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 459, number of negative: 171826\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17366\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 75\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002664 -> initscore=-5.925187\n[LightGBM] [Info] Start training from score -5.925187\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"array([[9.99660419e-01, 3.39581101e-04],\n       [9.98460380e-01, 1.53961976e-03],\n       [9.99903014e-01, 9.69859649e-05],\n       ...,\n       [9.99123428e-01, 8.76571640e-04],\n       [9.99891064e-01, 1.08936003e-04],\n       [9.98763597e-01, 1.23640349e-03]])"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve\nmd_predictions_probs = md_predictions_probs[:, 1]\nmd_roc = roc_auc_score(Y_validate, md_predictions_probs)\nmd_roc","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:58.168115Z","iopub.execute_input":"2024-11-10T06:40:58.168466Z","iopub.status.idle":"2024-11-10T06:40:58.203028Z","shell.execute_reply.started":"2024-11-10T06:40:58.168431Z","shell.execute_reply":"2024-11-10T06:40:58.201790Z"},"trusted":true},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"0.962348771844067"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"# Ensure X_train has columns\nX_train = pd.DataFrame(X_train, columns=X.columns)\n# Now this should work\nfeature_imp = pd.Series(lgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nprint(feature_imp)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:58.207305Z","iopub.execute_input":"2024-11-10T06:40:58.207691Z","iopub.status.idle":"2024-11-10T06:40:58.220090Z","shell.execute_reply.started":"2024-11-10T06:40:58.207655Z","shell.execute_reply":"2024-11-10T06:40:58.218789Z"},"trusted":true},"outputs":[{"name":"stdout","text":"X15    293\nX69    278\nX3     264\nX9     252\nX51    238\n      ... \nX19     20\nX77     19\nX72     17\nX16      3\nX4       2\nLength: 75, dtype: int32\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# doing for the actual test set\ntest_set_path = '/kaggle/input/iml-fall-2024-challenge-1/test_set.csv'\ntest_set = pd.read_csv(test_set_path)\nrecord_ids = test_set['RecordId']\ntest_set.drop(columns=['RecordId','X71','X76'], inplace=True)\nimr = SimpleImputer(missing_values=np.nan, strategy='mean')\ntest_set_imputed = imr.fit_transform(test_set)\nmms = MinMaxScaler()\ntest_set_norm = mms.fit_transform(test_set_imputed)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:58.223628Z","iopub.execute_input":"2024-11-10T06:40:58.224065Z","iopub.status.idle":"2024-11-10T06:40:59.981412Z","shell.execute_reply.started":"2024-11-10T06:40:58.224027Z","shell.execute_reply":"2024-11-10T06:40:59.980235Z"},"trusted":true},"outputs":[],"execution_count":54},{"cell_type":"code","source":"test_predictions_probs = lgb_model.predict_proba(test_set_norm)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:40:59.982650Z","iopub.execute_input":"2024-11-10T06:40:59.983045Z","iopub.status.idle":"2024-11-10T06:41:02.700926Z","shell.execute_reply.started":"2024-11-10T06:40:59.983005Z","shell.execute_reply":"2024-11-10T06:41:02.699662Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'RecordId' : record_ids,\n    'Y' : test_predictions_probs\n})\nsubmission.to_csv('submission7.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T06:41:02.702252Z","iopub.execute_input":"2024-11-10T06:41:02.702589Z","iopub.status.idle":"2024-11-10T06:41:02.977238Z","shell.execute_reply.started":"2024-11-10T06:41:02.702556Z","shell.execute_reply":"2024-11-10T06:41:02.976040Z"},"trusted":true},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"NOTE: this code doesnt return my highest accuracy which is 0.95634 but 0.95414. i made some changes and forgot to save the old ones so i lost the value of some parameters. regardless, this returns accuracy of 95.414% but please mark me according to the highest accuracy i achieved which was 95.634% as you will see in my submissions on kaggle and on google form. thank you.\n\nData Preparation and Preprocessing:\nDropped Features: Excluded RecordId, X71, and X76 as they seemed irrelevant according to algorithm specific feature importance algorithm, potentially reducing noise and improving the model's performance.\nImputation: Applied mean imputation for missing values to maintain continuity in the data without bias.\nFeature Scaling: Used MinMaxScaler to normalize features to [0,1], stabilizing the model and helping with consistent performance across algorithms.\n\nTrain-Validation Split:\nCreated a 70-30 train-validation split to evaluate generalization. Given the potential class imbalance, cross-validation.\n\nModel Comparisons:\nLightGBM (Best Model):\nLightGBM yielded the best results overall, especially with adjustments to parameters like colsample_bytree, reg_alpha, reg_lambda, and a refined learning rate (0.025) to three decimal places. These precise parameter adjustments were highly effective and balanced performance with efficiency.\nLightGBM’s training time and accuracy surpassed other models, making it an optimal choice.\n\nXGBoost:\nDespite tuning many parameters, including max_depth, n_estimators, min_child_weight, and subsample, XGBoost showed minimal improvement compared to LightGBM. While robust, XGBoost was less responsive to parameter adjustments, making it less efficient than LightGBM.\n\nVoting and Stacking Models:\nVoting and stacking methods consumed extensive resources, yet they didn’t yield a noticeable improvement over LightGBM. This resource-intensive nature made them less practical given their modest performance gain.\n\nExtra Trees and Bagging:\nBoth methods were effective, though heavily dependent on a high number of estimators, which made training time significantly longer than LightGBM. While reliable, they weren’t as efficient.\n\nCatBoost and AdaBoost:\nBoth models performed well but required a high number of iterations for strong results, leading to longer training times. Despite the reliable outcomes, the efficiency was outmatched by LightGBM.\n\nK-Nearest Neighbors (KNN):\nKNN was the least effective, requiring a high number of neighbors to perform well but delivering poor accuracy regardless. It was particularly time-consuming and inefficient.\n\nRandom Forest vs. Decision Trees:\nRandom Forest delivered strong results, yet it was more time-intensive than Decision Trees. Decision Trees, while simpler, were faster and delivered competitive accuracy. However, neither matched LightGBM’s balance of speed and accuracy.\n\nGradient Boosting and Naive Bayes:\nBoth models showed modest results. Gradient Boosting was fairly effective, but even with parameter tuning, it couldn’t surpass LightGBM. Naive Bayes performed quickly but with limited effectiveness on this dataset.\n\nEvaluation Metrics:\nROC-AUC Score: Used roc_auc_score on the validation set to objectively compare models, with LightGBM achieving the highest ROC-AUC.\nFeature Importance: For LightGBM, feature importance analysis highlighted top predictors, aiding further model optimization through targeted feature engineering.\n","metadata":{}}]}